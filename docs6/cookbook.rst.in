Cookbook part 1: common patterns
================================================================

Data-cleaning examples
----------------------------------------------------------------

Here are some ways to use the type-checking options as described in :ref:`reference-dsl-type-tests-and-assertions` Suppose you have the following data file, with inconsistent typing for boolean. (Also imagine that, for the sake of discussion, we have a million-line file rather than a four-line file, so we can't see it all at once and some automation is called for.)

GENRST_RUN_COMMAND
cat data/het-bool.csv
GENRST_EOF

One option is to coerce everything to boolean, or integer:

GENRST_RUN_COMMAND
mlr --icsv --opprint put '$reachable = boolean($reachable)' data/het-bool.csv
GENRST_EOF

GENRST_RUN_COMMAND
mlr --icsv --opprint put '$reachable = int(boolean($reachable))' data/het-bool.csv
GENRST_EOF

A second option is to flag badly formatted data within the output stream:

GENRST_RUN_COMMAND
mlr --icsv --opprint put '$format_ok = is_string($reachable)' data/het-bool.csv
GENRST_EOF

Or perhaps to flag badly formatted data outside the output stream:

GENRST_RUN_COMMAND
mlr --icsv --opprint put '
  if (!is_string($reachable)) {eprint "Malformed at NR=".NR}
' data/het-bool.csv
GENRST_EOF

A third way is to abort the process on first instance of bad data:

GENRST_RUN_COMMAND_TOLERATING_ERROR
mlr --csv put '$reachable = asserting_string($reachable)' data/het-bool.csv
GENRST_EOF

Showing differences between successive queries
----------------------------------------------------------------

Suppose you have a database query which you run at one point in time, producing the output on the left, then again later producing the output on the right:

GENRST_RUN_COMMAND
cat data/previous_counters.csv
GENRST_EOF

GENRST_RUN_COMMAND
cat data/current_counters.csv
GENRST_EOF

And, suppose you want to compute the differences in the counters between adjacent keys. Since the color names aren't all in the same order, nor are they all present on both sides, we can't just paste the two files side-by-side and do some column-four-minus-column-two arithmetic.

First, rename counter columns to make them distinct:

GENRST_RUN_COMMAND
mlr --csv rename count,previous_count data/previous_counters.csv > data/prevtemp.csv
GENRST_EOF

GENRST_RUN_COMMAND
cat data/prevtemp.csv
GENRST_EOF

GENRST_RUN_COMMAND
mlr --csv rename count,current_count data/current_counters.csv > data/currtemp.csv
GENRST_EOF

GENRST_RUN_COMMAND
cat data/currtemp.csv
GENRST_EOF

Then, join on the key field(s), and use unsparsify to zero-fill counters absent on one side but present on the other. Use ``--ul`` and ``--ur`` to emit unpaired records (namely, purple on the left and yellow on the right):

GENRST_INCLUDE_AND_RUN_ESCAPED(data/previous-to-current.sh)

Two-pass algorithms
----------------------------------------------------------------

Miller is a streaming record processor; commands are performed once per record. This makes Miller particularly suitable for single-pass algorithms, allowing many of its verbs to process files that are (much) larger than the amount of RAM present in your system. (Of course, Miller verbs such as ``sort``, ``tac``, etc. all must ingest and retain all input records before emitting any output records.) You can also use out-of-stream variables to perform multi-pass computations, at the price of retaining all input records in memory.

Two-pass algorithms: computation of percentages
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For example, mapping numeric values down a column to the percentage between their min and max values is two-pass: on the first pass you find the min and max values, then on the second, map each record's value to a percentage.

GENRST_INCLUDE_AND_RUN_ESCAPED(data/two-pass-percentage.sh)

Two-pass algorithms: line-number ratios
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Similarly, finding the total record count requires first reading through all the data:

GENRST_INCLUDE_AND_RUN_ESCAPED(data/two-pass-record-numbers.sh)

Two-pass algorithms: records having max value
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The idea is to retain records having the largest value of ``n`` in the following data:

GENRST_RUN_COMMAND
mlr --itsv --opprint cat data/maxrows.tsv
GENRST_EOF

Of course, the largest value of ``n`` isn't known until after all data have been read. Using an out-of-stream variable we can retain all records as they are read, then filter them at the end:

GENRST_RUN_COMMAND
cat data/maxrows.mlr
GENRST_EOF

GENRST_RUN_COMMAND
mlr --itsv --opprint put -q -f data/maxrows.mlr data/maxrows.tsv
GENRST_EOF

Feature-counting
----------------------------------------------------------------

Suppose you have some heterogeneous data like this:

GENRST_INCLUDE_ESCAPED(data/features.json)

A reasonable question to ask is, how many occurrences of each field are there? And, what percentage of total row count has each of them? Since the denominator of the percentage is not known until the end, this is a two-pass algorithm:

GENRST_INCLUDE_ESCAPED(data/feature-count.mlr)

Then

GENRST_RUN_COMMAND
mlr --json put -q -f data/feature-count.mlr data/features.json
GENRST_EOF

GENRST_RUN_COMMAND
mlr --ijson --opprint put -q -f data/feature-count.mlr data/features.json
GENRST_EOF

Unsparsing
----------------------------------------------------------------

The previous section discussed how to fill out missing data fields within CSV with full header line -- so the list of all field names is present within the header line. Next, let's look at a related problem: we have data where each record has various key names but we want to produce rectangular output having the union of all key names.

For example, suppose you have JSON input like this:

GENRST_RUN_COMMAND
cat data/sparse.json
GENRST_EOF

There are field names ``a``, ``b``, ``v``, ``u``, ``x``, ``w`` in the data -- but not all in every record.  Since we don't know the names of all the keys until we've read them all, this needs to be a two-pass algorithm. On the first pass, remember all the unique key names and all the records; on the second pass, loop through the records filling in absent values, then producing output. Use ``put -q`` since we don't want to produce per-record output, only emitting output in the ``end`` block:

GENRST_RUN_COMMAND
cat data/unsparsify.mlr
GENRST_EOF

GENRST_RUN_COMMAND
mlr --json put -q -f data/unsparsify.mlr data/sparse.json
GENRST_EOF

GENRST_RUN_COMMAND
mlr --ijson --ocsv put -q -f data/unsparsify.mlr data/sparse.json
GENRST_EOF

GENRST_RUN_COMMAND
mlr --ijson --opprint put -q -f data/unsparsify.mlr data/sparse.json
GENRST_EOF

There is a keystroke-saving verb for this: :ref:`mlr unsparsify <reference-verbs-unsparsify>`.

Parsing log-file output
----------------------------------------------------------------

This, of course, depends highly on what's in your log files. But, as an example, suppose you have log-file lines such as

GENRST_CARDIFY
2015-10-08 08:29:09,445 INFO com.company.path.to.ClassName @ [sometext] various/sorts/of data {& punctuation} hits=1 status=0 time=2.378
GENRST_EOF

I prefer to pre-filter with ``grep`` and/or ``sed`` to extract the structured text, then hand that to Miller. Example:

GENRST_SHOW_COMMAND
grep 'various sorts' *.log | sed 's/.*} //' | mlr --fs space --repifs --oxtab stats1 -a min,p10,p50,p90,max -f time -g status
GENRST_EOF

.. _cookbook-memoization-with-oosvars:

Memoization with out-of-stream variables
----------------------------------------------------------------

The recursive function for the Fibonacci sequence is famous for its computational complexity.  Namely, using f(0)=1, f(1)=1, f(n)=f(n-1)+f(n-2) for n>=2, the evaluation tree branches left as well as right at each non-trivial level, resulting in millions or more paths to the root 0/1 nodes for larger n. This program

GENRST_INCLUDE_ESCAPED(data/fibo-uncached.sh)

produces output like this:

GENRST_CARDIFY
i  o      fcount  seconds_delta
1  1      1       0
2  2      3       0.000039101
3  3      5       0.000015974
4  5      9       0.000019073
5  8      15      0.000026941
6  13     25      0.000036955
7  21     41      0.000056028
8  34     67      0.000086069
9  55     109     0.000134945
10 89     177     0.000217915
11 144    287     0.000355959
12 233    465     0.000506163
13 377    753     0.000811815
14 610    1219    0.001297235
15 987    1973    0.001960993
16 1597   3193    0.003417969
17 2584   5167    0.006215811
18 4181   8361    0.008294106
19 6765   13529   0.012095928
20 10946  21891   0.019592047
21 17711  35421   0.031193972
22 28657  57313   0.057254076
23 46368  92735   0.080307961
24 75025  150049  0.129482031
25 121393 242785  0.213325977
26 196418 392835  0.334423065
27 317811 635621  0.605969906
28 514229 1028457 0.971235037
GENRST_EOF

Note that the time it takes to evaluate the function is blowing up exponentially as the input argument increases. Using ``@``-variables, which persist across records, we can cache and reuse the results of previous computations:

GENRST_INCLUDE_ESCAPED(data/fibo-cached.sh)

with output like this:

GENRST_CARDIFY
i  o      fcount seconds_delta
1  1      1      0
2  2      3      0.000053883
3  3      3      0.000035048
4  5      3      0.000045061
5  8      3      0.000014067
6  13     3      0.000028849
7  21     3      0.000028133
8  34     3      0.000027895
9  55     3      0.000014067
10 89     3      0.000015020
11 144    3      0.000012875
12 233    3      0.000033140
13 377    3      0.000014067
14 610    3      0.000012875
15 987    3      0.000029087
16 1597   3      0.000013828
17 2584   3      0.000013113
18 4181   3      0.000012875
19 6765   3      0.000013113
20 10946  3      0.000012875
21 17711  3      0.000013113
22 28657  3      0.000013113
23 46368  3      0.000015974
24 75025  3      0.000012875
25 121393 3      0.000013113
26 196418 3      0.000012875
27 317811 3      0.000013113
28 514229 3      0.000012875
GENRST_EOF
